{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape de Web Ironman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def obtener_detalles(dir_url):\n",
    "    #print(\"      Obtenemos comentarios del anuncio\")\n",
    "    # spoof some headers so the request appears to be coming from a browser, not a bot\n",
    "\n",
    "    import sys\n",
    "    import requests\n",
    "    import time\n",
    "    from bs4 import BeautifulSoup\n",
    "    import xlsxwriter\n",
    "\n",
    "\n",
    "    headers = {\n",
    "        \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_5)\",\n",
    "        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"accept-charset\": \"ISO-8859-1,utf-8;q=0.7,*;q=0.3\",\n",
    "        \"accept-encoding\": \"gzip,deflate,sdch\",\n",
    "        \"accept-language\": \"en-US,en;q=0.8\",\n",
    "        }\n",
    "\n",
    "    # make the request to the search url, passing in the the spoofed headers.\n",
    "    r = requests.get(dir_url, headers=headers)  # assign the response to a variable r\n",
    "    #r = requests.get(\"http://eu.ironman.com/triathlon/events/americas/ironman/maryland/results.aspx?rd=20140920&race=maryland&bidid=52&detail=1#axzz4ttwHfpqw\", headers=headers)\n",
    "    # check the status code of the response to make sure the request went well\n",
    "    if r.status_code != 200:\n",
    "        print(\"request denied\")\n",
    "        print(r.status_code)\n",
    "        sigue = 0\n",
    "\n",
    "    #else:\n",
    "        #print(\"scraping \" + url)  \n",
    "\n",
    "\n",
    "    # convert the plaintext HTML markup into a DOM-like structure that we can search\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')# each result is an <li> element with class=\"g\" this is our wrapper\n",
    "\n",
    "    results = soup.findAll(\"table\", attrs={\"id\":\"general-info\"})\n",
    "    \n",
    "    ggee = ''\n",
    "    edad = ''\n",
    "    s_div_rank = ''\n",
    "    s_gender_rank = ''\n",
    "    s_overal_rank = ''\n",
    "    b_div_rank = ''\n",
    "    b_gender_rank = ''\n",
    "    b_overal_rank = ''\n",
    "    r_div_rank = ''\n",
    "    r_gender_rank = ''\n",
    "    r_overal_rank = ''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(results) == 0:\n",
    "        #print(\"No hay comentarios\")\n",
    "        comentario = \"\"\n",
    "\n",
    "    else:\n",
    "        # iterate over each of the result wrapper elements\n",
    "        \n",
    "        ggee = results[0].findAll(\"td\")[4].text.strip()\n",
    "        edad = results[0].findAll(\"td\")[6].text.strip()\n",
    "        \n",
    "\n",
    "    results = soup.findAll(\"div\", attrs={\"class\":\"athlete-table-details\"})\n",
    "\n",
    "    if len(results) == 0:\n",
    "        #print(\"No hay comentarios\")\n",
    "        comentario = \"\"\n",
    "\n",
    "    else:\n",
    "        # iterate over each of the result wrapper elements\n",
    "        s_div_rank = results[0].findAll(\"td\")[5].text.strip()\n",
    "        s_gender_rank = results[0].findAll(\"td\")[6].text.strip()\n",
    "        s_overal_rank = results[0].findAll(\"td\")[7].text.strip()\n",
    "        b_div_rank = results[0].findAll(\"td\")[13].text.strip()\n",
    "        b_gender_rank = results[0].findAll(\"td\")[14].text.strip()\n",
    "        b_overal_rank = results[0].findAll(\"td\")[15].text.strip()\n",
    "        r_div_rank = results[0].findAll(\"td\")[21].text.strip()\n",
    "        r_gender_rank = results[0].findAll(\"td\")[22].text.strip()\n",
    "        r_overal_rank = results[0].findAll(\"td\")[23].text.strip()\n",
    "    \n",
    "    detalle = (ggee, edad, s_div_rank, s_gender_rank, s_overal_rank, b_div_rank, b_gender_rank, b_overal_rank, r_div_rank, r_gender_rank, r_overal_rank)\n",
    "\n",
    "\n",
    "    #print(ggee, edad, s_div_rank, s_gender_rank, s_overal_rank, b_div_rank, b_gender_rank, b_overal_rank, r_div_rank, r_gender_rank, r_overal_rank)    \n",
    "    return detalle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagina: 1\n",
      "Pagina: 2\n",
      "Pagina: 3\n",
      "Pagina: 4\n",
      "Pagina: 5\n",
      "Pagina: 6\n",
      "Pagina: 7\n",
      "Pagina: 8\n",
      "Pagina: 9\n",
      "Pagina: 10\n",
      "Pagina: 11\n",
      "Pagina: 12\n",
      "Pagina: 13\n",
      "Pagina: 14\n",
      "Pagina: 15\n",
      "Pagina: 16\n",
      "Pagina: 17\n",
      "Pagina: 18\n",
      "Pagina: 19\n",
      "Pagina: 20\n",
      "Pagina: 21\n",
      "Pagina: 22\n",
      "Pagina: 23\n",
      "Pagina: 24\n",
      "Pagina: 25\n",
      "Pagina: 26\n",
      "Pagina: 27\n",
      "Pagina: 28\n",
      "Pagina: 29\n",
      "Pagina: 30\n",
      "Pagina: 31\n",
      "Pagina: 32\n",
      "Pagina: 33\n",
      "Pagina: 34\n",
      "Pagina: 35\n",
      "Pagina: 36\n",
      "Pagina: 37\n",
      "Pagina: 38\n",
      "Pagina: 39\n",
      "Pagina: 40\n",
      "Pagina: 41\n",
      "Pagina: 42\n",
      "Pagina: 43\n",
      "Pagina: 44\n",
      "Pagina: 45\n",
      "Pagina: 46\n",
      "Pagina: 47\n",
      "Pagina: 48\n",
      "Pagina: 49\n",
      "Pagina: 50\n",
      "Pagina: 51\n",
      "Pagina: 52\n",
      "Pagina: 53\n",
      "Pagina: 54\n",
      "Pagina: 55\n",
      "Pagina: 56\n",
      "Pagina: 57\n",
      "Pagina: 58\n",
      "Pagina: 59\n",
      "Pagina: 60\n",
      "Pagina: 61\n",
      "Pagina: 62\n",
      "Pagina: 63\n",
      "Pagina: 64\n",
      "Pagina: 65\n",
      "Paramos de recuperar páginas, la última no tiene datos\n",
      "       Iniciamos el recorrido del Loop\n",
      "Guardando.... 1265  filas\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import xlsxwriter\n",
    "\n",
    "\n",
    "ironmans = [[\"mar-del-plata\",\"americas\", \"20170312\"]\n",
    "            ]\n",
    "'''\n",
    "            \n",
    "            \n",
    "   \n",
    "           \n",
    "            #IM´s Europe\n",
    "            \n",
    "            [\"uk\",\"emea\", \"20170716\"],\n",
    "            [\"uk\",\"emea\", \"20160717\"],\n",
    "            [\"uk\",\"emea\", \"20150719\"],\n",
    "            [\"uk\",\"emea\", \"20140720\"],\n",
    "            [\"uk\",\"emea\", \"20130804\"],\n",
    "            [\"switzerland\",\"emea\", \"20170730\"],\n",
    "            [\"switzerland\",\"emea\", \"20160724\"],\n",
    "            [\"switzerland\",\"emea\", \"20150719\"],\n",
    "            [\"switzerland\",\"emea\", \"20140727\"],\n",
    "            [\"switzerland\",\"emea\", \"20130728\"],\n",
    "            [\"hamburg\",\"emea\", \"20170813\"],\n",
    "            [\"maastricht\",\"emea\", \"20170806\"],\n",
    "            [\"maastricht\",\"emea\", \"20160731\"],\n",
    "            [\"maastricht\",\"emea\", \"20150802\"],\n",
    "            [\"kalmar\",\"emea\", \"20170819\"],\n",
    "            [\"kalmar\",\"emea\", \"20160820\"],\n",
    "            [\"kalmar\",\"emea\", \"20150815\"],\n",
    "            [\"kalmar\",\"emea\", \"20140816\"],\n",
    "            [\"kalmar\",\"emea\", \"20130817\"],\n",
    "            [\"vichy\",\"emea\", \"20170827\"],\n",
    "            [\"vichy\",\"emea\", \"20160828\"],\n",
    "            [\"vichy\",\"emea\", \"20150830\"],\n",
    "            [\"wales\",\"emea\", \"20170910\"],\n",
    "            [\"wales\",\"emea\", \"20160918\"],\n",
    "            [\"wales\",\"emea\", \"20150913\"],\n",
    "            [\"wales\",\"emea\", \"20140914\"],\n",
    "            [\"wales\",\"emea\", \"20130908\"]\n",
    "            [\"frankfurt\",\"emea\", \"20170709\"],\n",
    "            [\"frankfurt\",\"emea\", \"20160703\"],\n",
    "            [\"frankfurt\",\"emea\", \"20150705\"],\n",
    "            [\"frankfurt\",\"emea\", \"20140706\"],\n",
    "            [\"frankfurt\",\"emea\", \"20130707\"],\n",
    "            [\"austria\",\"emea\", \"20170702\"],\n",
    "            [\"austria\",\"emea\", \"20160626\"],\n",
    "            [\"austria\",\"emea\", \"20150628\"],\n",
    "            [\"austria\",\"emea\", \"20140629\"],\n",
    "            [\"austria\",\"emea\", \"20130630\"],\n",
    "            [\"copenhagen\",\"emea\", \"20170820\"],\n",
    "            [\"copenhagen\",\"emea\", \"20160821\"],\n",
    "            [\"copenhagen\",\"emea\", \"20150823\"],\n",
    "            [\"copenhagen\",\"emea\", \"20140824\"],\n",
    "            [\"copenhagen\",\"emea\", \"20130818\"],\n",
    "            [\"Barcelona\",\"emea\", \"20151004\"],\n",
    "            [\"Barcelona\",\"emea\", \"20161002\"],\n",
    "            [\"Barcelona\",\"emea\", \"20141005\"],\n",
    "            [\"Barcelona\",\"emea\", \"20170930\"],       \n",
    "            [\"Lanzarote\",\"emea\", \"20170520\"],\n",
    "            [\"Lanzarote\",\"emea\", \"20160521\"],\n",
    "            [\"Lanzarote\",\"emea\", \"20150523\"],\n",
    "            [\"Lanzarote\",\"emea\", \"20140517\"],\n",
    "            [\"Lanzarote\",\"emea\", \"20130518\"],\n",
    "            [\"france\",\"emea\", \"20170723\"],\n",
    "            [\"france\",\"emea\", \"20160605\"],\n",
    "            [\"france\",\"emea\", \"20150628\"],\n",
    "            [\"france\",\"emea\", \"20140629\"],\n",
    "            [\"france\",\"emea\", \"20130623\"]\n",
    "            [\"emilia-romagna\",\"emea\", \"20170923\"],\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #Championships\n",
    "            \n",
    "            [[\"world-championship\",\"americas\", \"20171014\"],\n",
    "            [\"world-championship\",\"americas\", \"20161008\"],\n",
    "            [\"world-championship\",\"americas\", \"20151010\"],\n",
    "            [\"world-championship\",\"americas\", \"20141011\"],\n",
    "            [\"world-championship\",\"americas\", \"20131012\"],]\n",
    "            \n",
    "            #USA\n",
    "            [\"texas\",\"americas\", \"20170422\"],\n",
    "            [\"texas\",\"americas\", \"20160514\"],\n",
    "            [\"texas\",\"americas\", \"20150516\"],\n",
    "            [\"texas\",\"americas\", \"20140517\"],\n",
    "            [\"texas\",\"americas\", \"20130518\"],\n",
    "            [\"santa-rosa\",\"americas\", \"20170929\"],\n",
    "            [\"santa-rosa\",\"americas\", \"20160101\"],\n",
    "            [\"santa-rosa\",\"americas\", \"20150101\"],\n",
    "            [\"santa-rosa\",\"americas\", \"20140101\"],\n",
    "            [\"santa-rosa\",\"americas\", \"20130101\"],\n",
    "            [\"boulder\",\"americas\", \"20170611\"],\n",
    "            [\"boulder\",\"americas\", \"20160807\"],\n",
    "            [\"boulder\",\"americas\", \"20150802\"],\n",
    "            [\"boulder\",\"americas\", \"20140803\"],\n",
    "            [\"boulder\",\"americas\", \"20130101\"],\n",
    "            [\"lake-placid\",\"americas\", \"20170723\"],\n",
    "            [\"lake-placid\",\"americas\", \"20160724\"],\n",
    "            [\"lake-placid\",\"americas\", \"20150726\"],\n",
    "            [\"lake-placid\",\"americas\", \"20140727\"],\n",
    "            [\"lake-placid\",\"americas\", \"20130728\"],\n",
    "            [\"canada\",\"americas\", \"20170730\"],\n",
    "            [\"canada\",\"americas\", \"20160724\"],\n",
    "            [\"canada\",\"americas\", \"20150726\"],\n",
    "            [\"canada\",\"americas\", \"20140727\"],\n",
    "            [\"canada\",\"americas\", \"20130825\"],\n",
    "            [\"mont-tremblant\",\"americas\", \"20170820\"],\n",
    "            [\"mont-tremblant\",\"americas\", \"20160821\"],\n",
    "            [\"mont-tremblant\",\"americas\", \"20150816\"],\n",
    "            [\"mont-tremblant\",\"americas\", \"20140817\"],\n",
    "            [\"mont-tremblant\",\"americas\", \"20130818\"],\n",
    "            [\"wisconsin\",\"americas\", \"20170910\"],\n",
    "            [\"wisconsin\",\"americas\", \"20160911\"],\n",
    "            [\"wisconsin\",\"americas\", \"20150913\"],\n",
    "            [\"wisconsin\",\"americas\", \"20140907\"],\n",
    "            [\"wisconsin\",\"americas\", \"20130908\"],\n",
    "            [\"maryland\",\"americas\", \"20171007\"],\n",
    "            [\"maryland\",\"americas\", \"20161001\"],\n",
    "            [\"maryland\",\"americas\", \"20151017\"],\n",
    "            [\"maryland\",\"americas\", \"20140920\"],\n",
    "            [\"chattanooga\",\"americas\", \"20170924\"],\n",
    "            [\"chattanooga\",\"americas\", \"20160925\"],\n",
    "            [\"chattanooga\",\"americas\", \"20150927\"],\n",
    "            [\"chattanooga\",\"americas\", \"20140928\"],\n",
    "            [\"louisville\",\"americas\", \"20171015\"],\n",
    "            [\"louisville\",\"americas\", \"20161009\"],\n",
    "            [\"louisville\",\"americas\", \"20151011\"],\n",
    "            [\"louisville\",\"americas\", \"20140824\"],\n",
    "            [\"louisville\",\"americas\", \"20130825\"],\n",
    "            [\"florida\",\"americas\", \"20171104\"],\n",
    "            [\"florida\",\"americas\", \"20161105\"],\n",
    "            [\"florida\",\"americas\", \"20151107\"],\n",
    "            [\"florida\",\"americas\", \"20141101\"],\n",
    "            [\"florida\",\"americas\", \"20131102\"],\n",
    "            [\"cozumel\",\"americas\", \"20171126\"],\n",
    "            [\"cozumel\",\"americas\", \"20161127\"],\n",
    "            [\"cozumel\",\"americas\", \"20151129\"],\n",
    "            [\"cozumel\",\"americas\", \"20141130\"],\n",
    "            [\"cozumel\",\"americas\", \"20131201\"],\n",
    "            [\"arizona\",\"americas\", \"20171119\"],\n",
    "            [\"arizona\",\"americas\", \"20161120\"],\n",
    "            [\"arizona\",\"americas\", \"20151115\"],\n",
    "            [\"arizona\",\"americas\", \"20141116\"],\n",
    "            [\"arizona\",\"americas\", \"20131117\"],\n",
    "            \n",
    "            #AFRICA&OCEANIA\n",
    "            [\"south-africa\",\"emea\", \"20170402\"],\n",
    "            [\"south-africa\",\"emea\", \"20160410\"],\n",
    "            [\"south-africa\",\"emea\", \"20150329\"],\n",
    "            [\"south-africa\",\"emea\", \"20140406\"],\n",
    "            [\"south-africa\",\"emea\", \"20130414\"],\n",
    "            [\"new-zealand\",\"asiapac\", \"20170304\"],\n",
    "            [\"new-zealand\",\"asiapac\", \"20160305\"],\n",
    "            [\"new-zealand\",\"asiapac\", \"20150307\"],\n",
    "            [\"new-zealand\",\"asiapac\", \"20140301\"],\n",
    "            [\"new-zealand\",\"asiapac\", \"20130302\"],\n",
    "            [\"australia\",\"asiapac\", \"20170507\"],\n",
    "            [\"australia\",\"asiapac\", \"20160501\"],\n",
    "            [\"australia\",\"asiapac\", \"20150503\"],\n",
    "            [\"australia\",\"asiapac\", \"20140504\"],\n",
    "            [\"australia\",\"asiapac\", \"20130505\"],\n",
    "            [\"cairns\",\"asiapac\", \"20170611\"],\n",
    "            [\"cairns\",\"asiapac\", \"20160612\"],\n",
    "            [\"cairns\",\"asiapac\", \"20150614\"],\n",
    "            [\"cairns\",\"asiapac\", \"20140608\"],\n",
    "            [\"cairns\",\"asiapac\", \"20130609\"],\n",
    "            [\"western-australia\",\"asiapac\", \"20171203\"],\n",
    "            [\"western-australia\",\"asiapac\", \"20161204\"],\n",
    "            [\"western-australia\",\"asiapac\", \"20151206\"],\n",
    "            [\"western-australia\",\"asiapac\", \"20141207\"],\n",
    "            [\"western-australia\",\"asiapac\", \"20131208\"]\n",
    "            \n",
    "            #LATAM\n",
    "            [\"brazil\",\"americas\", \"20170528\"],\n",
    "            [\"brazil\",\"americas\", \"20160529\"],\n",
    "            [\"brazil\",\"americas\", \"20150531\"],\n",
    "            [\"brazil\",\"americas\", \"20140525\"],\n",
    "            [\"brazil\",\"americas\", \"20130526\"],\n",
    "            [\"mar-del-plata\",\"americas\", \"20170312\"]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "'''\n",
    "for races in ironmans:\n",
    "    \n",
    "    \n",
    "    pagina = 0\n",
    "    sigue = 1\n",
    "\n",
    "    lista_datos = []\n",
    "\n",
    "    while sigue == 1:\n",
    "    #while pagina < 2:\n",
    "        pagina += 1\n",
    "        print(\"Pagina: %s\" % pagina)\n",
    "\n",
    "        # dynamically build the URL that we'll be making a request to\n",
    "        #url = \"http://eu.ironman.com/triathlon/events/\"+races[1]+\"/ironman/\"+races[0]+\"/results.aspx?p=\"+str(pagina)+\"&rd=\"+races[2]\n",
    "        url = \"http://eu.ironman.com/triathlon/events/\"+races[1]+\"/ironman/\"+races[0]+\"/results.aspx?p=\"+str(pagina)\n",
    "        \n",
    "        #url = \"http://eu.ironman.com/triathlon/events/asiapac/ironman/new-zealand/results.aspx?p=70&rd=20170304\"\n",
    "        \n",
    "        # spoof some headers so the request appears to be coming from a browser, not a bot\n",
    "        headers = {\n",
    "            \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_5)\",\n",
    "            \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "            \"accept-charset\": \"ISO-8859-1,utf-8;q=0.7,*;q=0.3\",\n",
    "            \"accept-encoding\": \"gzip,deflate,sdch\",\n",
    "            \"accept-language\": \"en-US,en;q=0.8\",\n",
    "            }\n",
    "\n",
    "        # make the request to the search url, passing in the the spoofed headers.\n",
    "        r = requests.get(url, headers=headers)  # assign the response to a variable r\n",
    "\n",
    "        # check the status code of the response to make sure the request went well\n",
    "        if r.status_code != 200:\n",
    "            print(\"request denied\")\n",
    "            print(r.status_code)\n",
    "            sigue = 0\n",
    "\n",
    "        #else:\n",
    "            #print(\"scraping \" + url)  \n",
    "\n",
    "\n",
    "        # convert the plaintext HTML markup into a DOM-like structure that we can search\n",
    "        soup = BeautifulSoup(r.text, 'html5lib')# each result is an <li> element with class=\"g\" this is our wrapper\n",
    "\n",
    "\n",
    "        data_table = soup.findAll(\"tbody\")\n",
    "\n",
    "        #print(len(data_table))\n",
    "        #print(url)\n",
    "        #print(data_table)\n",
    "        \n",
    "        \n",
    "        #Para pruebas\n",
    "        #if len(data_table) == 0 or pagina == 2:\n",
    "\n",
    "        if len(data_table) == 0:\n",
    "            sigue = 0\n",
    "            print(\"Paramos de recuperar páginas, la última no tiene datos\")\n",
    "\n",
    "        else:\n",
    "            results = data_table[0].findAll(\"tr\")\n",
    "            # iterate over each of the result wrapper elements\n",
    "            for result in results:\n",
    "                data = result.findAll(\"td\")\n",
    "                name = data[0].find(\"a\").text\n",
    "                url = \"http://eu.ironman.com/triathlon/events/\"+races[1]+\"/ironman/\"+races[0]+\"/results.aspx\"+data[0].find(\"a\", attrs={\"class\":\"athlete\"}).attrs[\"href\"]\n",
    "                #url = \"http://eu.ironman.com/triathlon/events/\"+races[1]+\"/ironman/\"+races[0]+\"/results.aspx?p=\"+str(pagina)\n",
    "                #url = \"http://eu.ironman.com/triathlon/events/asiapac/ironman/new-zealand/results.aspx?p=70&rd=20170304\"\n",
    "        \n",
    "                \n",
    "                detalle = obtener_detalles(url)\n",
    "                \n",
    "                country = data[1].text\n",
    "                div_rank = data[2].text\n",
    "                gender_rank =data[3].text\n",
    "                overall_rank = data[4].text\n",
    "                swim = data[5].text\n",
    "                bike = data[6].text\n",
    "                run = data[7].text\n",
    "                finish = data[8].text\n",
    "                points = data[9].text\n",
    "                ggee = detalle[0]\n",
    "                edad = detalle[1]\n",
    "                s_div_rank = detalle[2]\n",
    "                s_gender_rank = detalle[3]\n",
    "                s_overal_rank = detalle[4]\n",
    "                b_div_rank = detalle[5]\n",
    "                b_gender_rank = detalle[6]\n",
    "                b_overal_rank = detalle[7]\n",
    "                r_div_rank = detalle[8]\n",
    "                r_gender_rank = detalle[9]\n",
    "                r_overal_rank = detalle[10]\n",
    "                race = races[0]\n",
    "                date = races[2]\n",
    "                \n",
    "                              \n",
    "                lista_datos.append([name,url,country,div_rank,gender_rank,overall_rank,swim,bike,run,finish,\n",
    "                                    points, ggee, edad, s_div_rank, s_gender_rank, s_overal_rank, b_div_rank, \n",
    "                                    b_gender_rank, b_overal_rank, r_div_rank, r_gender_rank, r_overal_rank, race,date])\n",
    "\n",
    "    #Escribimos el fichero de salida\n",
    "    workbook = xlsxwriter.Workbook(\"Datos_Iroman_\"+race+\"_\"+date+\"_detail.xlsx\")\n",
    "    worksheet = workbook.add_worksheet()\n",
    "    # Start from the first cell. Rows and columns are zero indexed.\n",
    "    row = 1\n",
    "    col = 0\n",
    "    worksheet.write(0, col,     \"name\")\n",
    "    worksheet.write(0, col + 1, \"url\")\n",
    "    worksheet.write(0, col + 2, \"country\")\n",
    "    worksheet.write(0, col + 3, \"div_rank\")\n",
    "    worksheet.write(0, col + 4, \"gender_rank\")\n",
    "    worksheet.write(0, col + 5, \"overall_rank\")\n",
    "    worksheet.write(0, col + 6, \"swim\")\n",
    "    worksheet.write(0, col + 7, \"bike\")\n",
    "    worksheet.write(0, col + 8, \"run\")\n",
    "    worksheet.write(0, col + 9, \"finish\")\n",
    "    worksheet.write(0, col + 10, \"points\")\n",
    "    worksheet.write(0, col + 11, \"ggee\")\n",
    "    worksheet.write(0, col + 12, \"edad\")\n",
    "    worksheet.write(0, col + 13, \"s_div_rank\")\n",
    "    worksheet.write(0, col + 14, \"s_gender_rank\")    \n",
    "    worksheet.write(0, col + 15, \"s_overal_rank\")\n",
    "    worksheet.write(0, col + 16, \"b_div_rank\")\n",
    "    worksheet.write(0, col + 17, \"b_gender_rank\")    \n",
    "    worksheet.write(0, col + 18, \"b_overal_rank\")\n",
    "    worksheet.write(0, col + 19, \"r_div_rank\")\n",
    "    worksheet.write(0, col + 20, \"r_gender_rank\")    \n",
    "    worksheet.write(0, col + 21, \"r_overal_rank\")\n",
    "    worksheet.write(0, col + 22, \"race\")\n",
    "    worksheet.write(0, col + 23, \"date\")\n",
    "\n",
    "\n",
    "    print(\"       Iniciamos el recorrido del Loop\")\n",
    "\n",
    "    # Iterate over the data and write it out row by row.\n",
    "    for name,url,country,div_rank,gender_rank,overall_rank,swim,bike,run,finish,points,ggee,edad,s_div_rank,s_gender_rank,s_overal_rank,b_div_rank,b_gender_rank,b_overal_rank,r_div_rank,r_gender_rank,r_overal_rank, race, date in (lista_datos):\n",
    "       \n",
    "        worksheet.write(row, col,     name)\n",
    "        worksheet.write(row, col + 1, url)\n",
    "        worksheet.write(row, col + 2, country)\n",
    "        worksheet.write(row, col + 3, div_rank)\n",
    "        worksheet.write(row, col + 4, gender_rank)\n",
    "        worksheet.write(row, col + 5, overall_rank)\n",
    "        worksheet.write(row, col + 6, swim)\n",
    "        worksheet.write(row, col + 7, bike)\n",
    "        worksheet.write(row, col + 8, run)\n",
    "        worksheet.write(row, col + 9, finish)\n",
    "        worksheet.write(row, col + 10, points)\n",
    "        worksheet.write(row, col + 11, ggee)\n",
    "        worksheet.write(row, col + 12, edad)\n",
    "        worksheet.write(row, col + 13, s_div_rank)\n",
    "        worksheet.write(row, col + 14, s_gender_rank)    \n",
    "        worksheet.write(row, col + 15, s_overal_rank)\n",
    "        worksheet.write(row, col + 16, b_div_rank)\n",
    "        worksheet.write(row, col + 17, b_gender_rank)    \n",
    "        worksheet.write(row, col + 18, b_overal_rank)\n",
    "        worksheet.write(row, col + 19, r_div_rank)\n",
    "        worksheet.write(row, col + 20, r_gender_rank)    \n",
    "        worksheet.write(row, col + 21, r_overal_rank)\n",
    "        worksheet.write(row, col + 22, race)\n",
    "        worksheet.write(row, col + 23, date)\n",
    "\n",
    "        row += 1\n",
    "    num_filas = len(lista_datos)\n",
    "    print(\"Guardando.... %d  filas\" % num_filas)\n",
    "    workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comprobación Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', '', '', '', '', '', '', '', '', '', '')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#obtener_detalles(\"http://eu.ironman.com/triathlon/events/asiapac/ironman/new-zealand/athletes/results.aspx?p=5&rd=20170304\")\n",
    "obtener_detalles(\"http://eu.ironman.com/triathlon/events/americas/ironman/mar-del-plata/results.aspx?p=7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tbody>\n",
       "                             <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=6&amp;detail=1\">Currie, Braden</a>    </td><td>NZL</td>    <td>1</td>    <td>1</td>    <td>1</td>    <td>00:48:11</td>    <td>04:39:55</td>    <td>02:48:23</td>    <td>08:20:58</td>    <td>5000</td></tr>\n",
       " <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=1&amp;detail=1\">Brown, Cameron</a>    </td><td>NZL</td>    <td>2</td>    <td>2</td>    <td>2</td>    <td>00:53:17</td>    <td>04:43:46</td>    <td>02:42:29</td>    <td>08:24:32</td>    <td>4957</td></tr>\n",
       " <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=5&amp;detail=1\">Viennot, Cyril</a>    </td><td>FRA</td>    <td>3</td>    <td>3</td>    <td>3</td>    <td>00:53:23</td>    <td>04:42:28</td>    <td>02:43:39</td>    <td>08:25:43</td>    <td>4943</td></tr>\n",
       " <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=2&amp;detail=1\">Albert, Marko</a>    </td><td>EST</td>    <td>4</td>    <td>4</td>    <td>4</td>    <td>00:48:01</td>    <td>04:41:47</td>    <td>02:56:03</td>    <td>08:30:16</td>    <td>4888</td></tr>\n",
       " <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=10&amp;detail=1\">Bowstead, Mark</a>    </td><td>NZL</td>    <td>5</td>    <td>5</td>    <td>5</td>    <td>00:51:07</td>    <td>04:40:36</td>    <td>02:54:24</td>    <td>08:31:01</td>    <td>4879</td></tr>\n",
       " <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=3&amp;detail=1\">Bozzone, Terenzo</a>    </td><td>NZL</td>    <td>6</td>    <td>6</td>    <td>6</td>    <td>00:48:08</td>    <td>04:37:45</td>    <td>03:03:07</td>    <td>08:33:35</td>    <td>4848</td></tr>\n",
       " <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=9&amp;detail=1\">Bittner, Per</a>    </td><td>DEU</td>    <td>7</td>    <td>7</td>    <td>7</td>    <td>00:53:22</td>    <td>04:43:50</td>    <td>03:02:08</td>    <td>08:44:13</td>    <td>4721</td></tr>\n",
       " <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=16&amp;detail=1\">Cochrane, Simon</a>    </td><td>NZL</td>    <td>8</td>    <td>8</td>    <td>8</td>    <td>00:53:10</td>    <td>04:48:49</td>    <td>03:01:36</td>    <td>08:48:10</td>    <td>4673</td></tr>\n",
       " <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=25&amp;detail=1\">Van Looy, Diego</a>    </td><td>BEL</td>    <td>9</td>    <td>9</td>    <td>9</td>    <td>01:07:48</td>    <td>04:47:44</td>    <td>02:50:50</td>    <td>08:52:05</td>    <td>4626</td></tr>\n",
       " <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=296&amp;detail=1\">Plews, Dan</a>    </td><td>NZL</td>    <td>1</td>    <td>10</td>    <td>10</td>    <td>00:54:05</td>    <td>04:58:55</td>    <td>02:55:11</td>    <td>08:54:10</td>    <td>5000</td></tr>\n",
       " <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=7&amp;detail=1\">Fettell, Clayton</a>    </td><td>AUS</td>    <td>10</td>    <td>11</td>    <td>11</td>    <td>00:48:03</td>    <td>04:46:47</td>    <td>03:15:23</td>    <td>08:54:47</td>    <td>4594</td></tr>\n",
       " <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=19&amp;detail=1\">Koutny, Philipp</a>    </td><td>CHE</td>    <td>11</td>    <td>12</td>    <td>12</td>    <td>00:53:22</td>    <td>04:51:01</td>    <td>03:15:10</td>    <td>09:04:57</td>    <td>4472</td></tr>\n",
       " <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=24&amp;detail=1\">Read, Carl</a>    </td><td>NZL</td>    <td>12</td>    <td>13</td>    <td>13</td>    <td>00:57:02</td>    <td>05:02:57</td>    <td>03:01:06</td>    <td>09:07:01</td>    <td>4447</td></tr>\n",
       " <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=139&amp;detail=1\">Dovey, Jackson</a>    </td><td>USA</td>    <td>1</td>    <td>14</td>    <td>14</td>    <td>00:55:22</td>    <td>05:01:21</td>    <td>03:04:38</td>    <td>09:07:38</td>    <td>5000</td></tr>\n",
       " <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=45&amp;detail=1\">Mccauley, Jocelyn</a>    </td><td>USA</td>    <td>1</td>    <td>1</td>    <td>15</td>    <td>00:59:14</td>    <td>05:05:24</td>    <td>02:59:40</td>    <td>09:09:47</td>    <td>5000</td></tr>\n",
       " <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=588&amp;detail=1\">Madgwick, Brodie</a>    </td><td>NZL</td>    <td>2</td>    <td>15</td>    <td>16</td>    <td>00:58:50</td>    <td>05:03:47</td>    <td>03:04:53</td>    <td>09:13:13</td>    <td>4771</td></tr>\n",
       " <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=208&amp;detail=1\">Wright, Jordy</a>    </td><td>AUS</td>    <td>2</td>    <td>16</td>    <td>17</td>    <td>00:59:49</td>    <td>05:01:57</td>    <td>03:08:48</td>    <td>09:17:17</td>    <td>4884</td></tr>\n",
       " <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=36&amp;detail=1\">Siddall, Laura</a>    </td><td>GBR</td>    <td>2</td>    <td>2</td>    <td>18</td>    <td>00:59:16</td>    <td>05:05:00</td>    <td>03:12:47</td>    <td>09:21:53</td>    <td>4854</td></tr>\n",
       " <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=31&amp;detail=1\">Kessler, Meredith</a>    </td><td>USA</td>    <td>3</td>    <td>3</td>    <td>19</td>    <td>00:53:23</td>    <td>05:10:54</td>    <td>03:18:11</td>    <td>09:27:19</td>    <td>4789</td></tr>\n",
       " <tr>\n",
       "     <td><a class=\"athlete\" href=\"?rd=20170304&amp;race=newzealand&amp;bidid=287&amp;detail=1\">Schoeman, Jason</a>    </td><td>NZL</td>    <td>3</td>    <td>17</td>    <td>20</td>    <td>01:11:51</td>    <td>05:00:36</td>    <td>03:09:00</td>    <td>09:27:27</td>    <td>4762</td></tr>\n",
       " \n",
       "                         </tbody>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
